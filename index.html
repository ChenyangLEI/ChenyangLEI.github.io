<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
 -->
  <title>Chenyang Lei (雷晨阳)</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chenyang Lei (雷晨阳)</name>
              </p>
              <p>Chenyang Lei is an assistant professor at CAIR, CAS. He is also a research scholar at Princeton University and working with Felix Heide. He received his Ph.D. in computer science from the Hong Kong University of Science and Technology (HKUST), supervised by <a href="https://cqf.io">Qifeng Chen</a>. He was a research intern at MSRA, Nvidia, and Sensetime. He obtained his Bachelor's degree at Zhejiang University in 2018.  
              </p>

              <p>I am looking for students, postdocs, and research assistants. If you are a student interested in doing research with me, please send me an email.</p>


              


              <p style="text-align:center">
                <a href="mailto:leichenyang7@gmail.com">Email</a> &nbsp/&nbsp
                <a href="images/Chenyang_Lei__CV.pdf">CV</a> &nbsp/&nbsp 
                <a href="https://scholar.google.com/citations?user=CuGF_pEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/%E6%99%A8%E9%98%B3-%E9%9B%B7-73565b149/">Linked In</a>  &nbsp/&nbsp 
                <a href="https://github.com/ChenyangLEI">Github</a> &nbsp/&nbsp 
                <a href="https://twitter.com/Thor78639964">Twitter</a>  

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/chenyanglei.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chenyanglei_Circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tbody>
                  <tr>
                    <td>
                      <!-- <heading> -->
                        <!-- <font color="black">News</font> -->
                      <!-- </heading> -->
                      <p>
                      <ul>
                        <li>News (Feb 2024): Four papers are accepted to CVPR 2024.</li>
                        <li>News (Dec 2023): A paper is accepted to AAAI 2024.</li>
                        <li>News (Aug 2023): A paper is accepted to TPAMI 2023.</li>
                        <li>News (Aug 2023): A paper is accepted to SIGGRAPH Asia 2023.</li>
                        <li>News (Jul 2023): Three papers are accepted to ICCV 2023.</li>
                        <li>News (Feb 2023): Two papers are accepted to CVPR 2023.</li>
                        <li>News (Sep 2022): I joined Princeton University as a research scholar, working with Felix Heide. </li>                        
                        <li>News (Sep 2022): I joined CAIR, HKISI-CAS as an assistant professor. </li>                        
                        <li>News (Aug 2022): I passed my Ph.D. defense!</li>
<!--                         <li>News (Apr 2021): I started an internship at Nvidia, supervised by Orazio Gallo, Abhishek Badki and Hang Su. </li> -->
                        <li>News (Mar 2022): One paper is accepted to CVPR 2022.</li>
                        <li>News (Dec 2021): One paper is accepted to TPAMI 2022.</li>
<!--                         <li>News (Apr 2021): I started an internship at MSRA, supervised by Steve Lin, Zhirong Wu and Xiao Sun. </li> -->
                        <li>News (Mar 2021): Two papers are accepted to CVPR 2021.</li>
                      </ul>
                      </p>
                    </td>
                  </tr>
                </tbody>


            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in exploring system designs of visual computing pipelines, from advanced sensing technologies to more capable artificial intelligence. 
              </p>

                My current research topics include:
              <ul>
                <li><strong>Computational imaging and photography</strong></li>
                <li><strong>Video processing, editing, and generation</strong></li>
                <li><strong>3D from sensors and multiviews</strong></li>
                <li><strong>Modality-agnostic foundation models</strong></li>
              </ul>

              <!-- <p>()</p> -->
            </td>

            
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <p>
              </p>
                            
            </td>

            
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




          <tr onmouseout="pollidar_stop()" onmouseover="pollidar_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pollidar_image'>
                  <img src='images/pollidar_after.png' width="160"></div>
                <img src='images/pollidar_before.png' width="160">
              </div>
              <script type="text/javascript">
                function pollidar_start() {
                  document.getElementById('pollidar_image').style.opacity = "1";
                }
          
                function pollidar_stop() {
                  document.getElementById('pollidar_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Polarization Wavefront Lidars: Learning to Recover Large-Scale Scene Information from Polarized Wavefronts
            </papertitle>
          </a>
          <br>
          Dominik Scheuble*,
          <strong>Chenyang Lei*</strong>,
          Mario Bijelic, Seung-Hwan Baek, 
          Felix Heide 
          <br>
          <em>CVPR</em>, 2024
          <br>
          arxiv / project website / code
          <p></p>
          <p>In this work, we introduce a novel long-range polarization wavefront lidar sensor (PolLidar) that modulates the polarization of the emitted and received light.</p>
          </td>
          </tr>  

          
          <tr onmouseout="pde_stop()" onmouseover="pde_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pde_image'>
                  <img src='images/pde_after.png' width="160"></div>
                <img src='images/pde_before.png' width="160">
              </div>
              <script type="text/javascript">
                function pde_start() {
                  document.getElementById('pde_image').style.opacity = "1";
                }
          
                function pde_stop() {
                  document.getElementById('pde_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Robust Depth Enhancement via Polarization Prompt Fusion Tuning</papertitle>
          </a>
          <br>
          Kei IKEMURA*, Yiming Huang*, Felix Heide, Zhaoxiang Zhang, <a href="http://cqf.io">Qifeng Chen,</a>         <strong>Chenyang Lei†</strong>
          <br>
          <em>CVPR</em>, 2024
          <br>
          arxiv / project website / code
          <p></p>
          <p></p>
          <p></p>

          <p>In this work, we present a general framework that leverages polarization imaging to improve inaccurate depth measurements from various depth sensors.</p>
          </td>
          </tr>  
          
            <tr onmouseout="imagination_stop()" onmouseover="imagination_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='imagination_image'>
                    <img src='images/imagination_after.png' width="160"></div>
                  <img src='images/imagination_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function imagination_start() {
                    document.getElementById('imagination_image').style.opacity = "1";
                  }
  
                  function imagination_stop() {
                    document.getElementById('imagination_image').style.opacity = "0";
                  }
                  videodepth_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Automatic Controllable Colorization by Imagination</papertitle>
                </a>
                <br>
                Xiaoyan Cong, Yue Wu,
                <a href="http://cqf.io">Qifeng Chen,</a>
                <strong>Chenyang Lei†</strong>,
                <br>
                  <em>CVPR </em>, 2024 
                <br>
                <!-- <a href=""></a>  -->
                arxiv / project website / code
                <p></p>
                <p>We present a novel approach to automatic image colorization by imitating the imagination process of human experts. </p>
              </td>
          </tr>  


          <tr onmouseout="nsf_stop()" onmouseover="nsf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nsf_image'>
                  <img src='images/nsf_after.png' width="160"></div>
                <img src='images/nsf_before.png' width="160">
              </div>
              <script type="text/javascript">
                function nsf_start() {
                  document.getElementById('nsf_image').style.opacity = "1";
                }
          
                function nsf_stop() {
                  document.getElementById('nsf_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Neural Spline Fields for Burst Image Fusion and Layer Separation</papertitle>
          </a>
          <br>
          Ilya Chugunov, David Shustin, Ruyu Yan,
          <strong>Chenyang Lei</strong>, Felix Heide
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="https://light.princeton.edu/wp-content/uploads/2023/12/neural-spline-fields.pdf">paper</a> / 
          <a href="https://light.princeton.edu/publication/nsf/">project website</a> / 
          <a href="https://github.com/princeton-computational-imaging/NSF">code</a> 
          <p></p>
          <p>In this work, we use burst image stacks for layer separation. We represent a burst of images with a two-layer alpha-composited image plus flow model constructed with neural spline fields networks trained to map input coordinates to spline control points.</p>
          </td>
          </tr> 

          <tr onmouseout="metalens_stop()" onmouseover="metalens_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='metalens_image'>
                  <img src='images/metalens-array.png' width="160"></div>
                <img src='images/metalens-array.png' width="160">
              </div>
              <script type="text/javascript">
                function metalens_start() {
                  document.getElementById('metalens_image').style.opacity = "1";
                }
          
                function metalens_stop() {
                  document.getElementById('metalens_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Thin On-Sensor Nanophotonic Array Cameras </papertitle>
          </a>
          <br>
          Praneeth Chakravarthula, Jipeng Sun, Xiao Li,
          <strong>Chenyang Lei</strong>,
          Gene Chou, Mario Bijelic, Johannes Froech, Arka Majumdar, Felix Heide
          <br>
          <em>SIGGRAPH Asia</em>, 2023
          <br>
          <a href="https://light.princeton.edu/wp-content/uploads/2023/11/Nano_Array_Cameras.pdf">paper</a> / 
          <a href="https://light.princeton.edu/publication/thin-on-sensor-nanophotonic-array-cameras/">project website</a>
          <p></p>
          <p>We propose a thin nanophotonic imager that employs a learned array of metalenses to capture a scene in-the-wild.</p>
          </td>
          </tr> 

          <tr onmouseout="augmentation_stop()" onmouseover="augmentation_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='augmentation_image'>
                  <img src='images/augmentation.png' width="160"></div>
                <img src='images/augmentation.png' width="160">
              </div>
              <script type="text/javascript">
                function augmentation_start() {
                  document.getElementById('augmentation_image').style.opacity = "1";
                }
          
                function augmentation_stop() {
                  document.getElementById('augmentation_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning </papertitle>
          </a>
          <br>
          Huimin Wu*,
          <strong>Chenyang Lei*</strong>,
          Xiao Sun, Peng-Shuai Wang, 
          <a href="http://cqf.io">Qifeng Chen,</a>
          Kwang-Ting Cheng, Stephen Lin, Zhirong Wu 
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2212.08663">arxiv</a> / 
          <a href="">project website</a> /
          <a href="https://github.com/microsoft/random_quantize">code</a>
          <p></p>
          <p>A modality-agnostic augmentation for constrative learning, which can be applied to image, audio, pointclouds, sensors and other modalities.</p>
          </td>
          </tr>  

          <tr onmouseout="fatezero_stop()" onmouseover="fatezero_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fatezero_image'>
                  <img src='images/fatezero.gif' width="160"></div>
                <img src='images/fatezero.gif' width="160">
              </div>
              <script type="text/javascript">
                function fatezero_start() {
                  document.getElementById('fatezero_image').style.opacity = "1";
                }
          
                function fatezero_stop() {
                  document.getElementById('fatezero_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>FateZero: Fusing Attentions for Zero-shot Text-based Video Editing  </papertitle>
          </a>
          <br>
          Chenyang Qi,
          Xiaodong Cun,
          Yong Zhang,
          <strong>Chenyang Lei</strong>,
          Xintao Wang,
          Ying Shan,
          <a href="http://cqf.io">Qifeng Chen</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2303.09535">arxiv</a> / 
          <a href="https://fate-zero-edit.github.io/">project website</a> /
          <a href="https://github.com/ChenyangQiQi/FateZero">code</a>
          <p></p>
          <p>A zero-shot text-driven video style and local attribute editing model.</p>
          </td>
          </tr>  

          <tr onmouseout="deflicker_stop()" onmouseover="deflicker_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deflicker_image'>
                  <img src='images/deflicker.png' width="160"></div>
                <img src='images/deflicker.png' width="160">
              </div>
              <script type="text/javascript">
                function deflicker_start() {
                  document.getElementById('deflicker_image').style.opacity = "1";
                }

                function deflicker_stop() {
                  document.getElementById('deflicker_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Blind Video Deflickering by Neural Filtering with a Flawed Atlas
            </papertitle>
          </a>
          <br>
          <strong>Chenyang Lei*</strong>,
          Xuanchi Ren*,
          Zhaoxiang Zhang,
          <a href="http://cqf.io">Qifeng Chen</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://arxiv.org/pdf/2303.08120.pdf">arxiv</a> / 
          <a href="deflicker/index.html">project website</a> /
          <a href="https://github.com/ChenyangLEI/All-in-one-Deflicker">code</a>
          <p></p>
          <p>We present a general postprocessing framework that can remove different types of flicker from various videos, including videos from video capturing, processing, and generation.</p>
        </td>
      </tr>   

          <tr onmouseout="inv3d_stop()" onmouseover="inv3d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='inv3d_image'>
                  <img src='images/HFGI3D1.png' width="160"></div>
                <img src='images/HFGI3D.png' width="160">
              </div>
              <script type="text/javascript">
                function inv3d_start() {
                  document.getElementById('inv3d_image').style.opacity = "1";
                }

                function inv3d_stop() {
                  document.getElementById('inv3d_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>High-fidelity 3D GAN Inversion by Pseudo-multi-view Optimization
            </papertitle>
          </a>
          <br>
          Jiaxin Xie*,
          Hao Ouyang*,
          Jingtan Piao,
          <strong>Chenyang Lei </strong>,
          <a href="http://cqf.io">Qifeng Chen </a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2211.15662">arxiv</a> / 
          <a href="https://ken-ouyang.github.io/HFGI3D/index.html">project website</a> /  
          <a href="https://github.com/jiaxinxie97/HFGI3D">code</a>  
          <p></p>
          <p>We present a high-fidelity 3D generative adversarial network (GAN) inversion framework that can synthesize photo-realistic novel views while preserving specific details of the input image.</p>
        </td>
      </tr>  


          <tr onmouseout="DVPvp_stop()" onmouseover="DVPvp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DVPvp_image'>
                  <img src='images/dvpvp_after.png' width="160"></div>
                <img src='images/dvpvp_before.png' width="160">
              </div>
              <script type="text/javascript">
                function DVPvp_start() {
                  document.getElementById('DVPvp_image').style.opacity = "1";
                }

                function DVPvp_stop() {
                  document.getElementById('DVPvp_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deep Video Prior for Video Consistency and Propagation</papertitle>
              </a>
              <br>
              <strong>Chenyang Lei</strong>,
              Yazhou Xing,
              Hao Ouyang, 
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>TPAMI </em>, 2022  
              <br>
              <a href="https://github.com/ChenyangLEI/deep-video-prior">code</a> 
              <p></p>
              <p>We extend the deep video prior (NeurIPS 2020) to video propagation. We also improve the training efficiency for deep video prior. </p>
            </td>
          </tr>  

          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sfp_image'>
                  <img src='images/sfp_after.png' width="160"></div>
                <img src='images/sfp_before.png' width="160">
              </div>
              <script type="text/javascript">
                function sfp_start() {
                  document.getElementById('sfp_image').style.opacity = "1";
                }

                function sfp_stop() {
                  document.getElementById('sfp_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Shape from Polarization for Complex Scenes in the Wild</papertitle>
              </a>
              <br>
              <strong>Chenyang Lei*</strong>,
              Chenyang Qi*,
              Jiaxin Xie*,
              Na Fan, 
              <a href="http://vladlen.info/">Vladlen Koltun </a>, 
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
              <a href="https://arxiv.org/pdf/2112.11377.pdf">arxiv</a> / 
              <a href="https://chenyanglei.github.io/sfpwild/index.html">project website</a> /  
              <a href="https://github.com/ChenyangLEI/sfp-wild">code</a>  

              <p></p>
              <p>We present a new data-driven approach with physics-based priors to scene-level normal estimation from a single polarization image. </p>
            </td>

          </tr>  


          <tr onmouseout="flash_stop()" onmouseover="flash_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flash_image'>
                  <img src='images/flash_after.jpg' width="160"></div>
                <img src='images/flash_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function flash_start() {
                  document.getElementById('flash_image').style.opacity = "1";
                }

                function flash_stop() {
                  document.getElementById('flash_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2010.11838"> -->
                <papertitle>Robust Reflection Removal with Reflection-free Flash-only Cues                </papertitle>
              </a>
              <br>
              <strong>Chenyang Lei</strong>,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>CVPR </em>, 2021  
              <br>
              <a href="https://arxiv.org/pdf/2103.04273.pdf">arxiv</a> / 
              <a href="https://github.com/ChenyangLEI/flash-reflection-removal">code</a> / 
              <a href="flashrr_rfc/index.html">project website</a> 
              <p></p>
              <p>We propose a simple yet effective reflection-free cue for robust reflection removal from a pair of flash and ambient (no-flash) images. 
                The reflection-free cue exploits a flash-only image obtained by subtracting the ambient image from the corresponding flash image in raw data space. 
                The flash-only image is equivalent to an image taken in a dark environment with only a flash on.  </p>
            </td>
          </tr>  
          <tr onmouseout="sim_stop()" onmouseover="sim_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim_image'>
                  <img src='images/simulator_after.PNG' width="160"></div>
                <img src='images/simulator_before.PNG' width="160">
              </div>
              <script type="text/javascript">
                function sim_start() {
                  document.getElementById('sim_image').style.opacity = "1";
                }

                function sim_stop() {
                  document.getElementById('sim_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2010.11838"> -->
                <papertitle>Neural Camera Simulators</papertitle>
              </a>
              <br>
              <a>Hao Ouyang*</a>,
              <a>Zifan Shi*</a>,
              <strong>Chenyang Lei</strong>,
              <a>Ka Lung Law</a>,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>CVPR </em>, 2021  

              <br>
              <a href="https://cqf.io/papers/Neural_Camera_Simulators_CVPR2021.pdf">paper</a>  /
              <a href="https://github.com/ken-ouyang/neural_image_simulator">code</a>  
              <!-- <a href="DVP/index.html">project website</a> -->
              <p></p>
              <p>We present a controllable camera simulator based on deep neural networks to synthesize raw image data under different camera settings, including exposure time, ISO, and aperture. </p>
            </td>
          </tr>  

          <tr onmouseout="DVP_stop()" onmouseover="DVP_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DVP_image'>
                  <img src='images/DVP_after.JPG' width="160"></div>
                <img src='images/DVP_before.JPG' width="160">
              </div>
              <script type="text/javascript">
                function DVP_start() {
                  document.getElementById('DVP_image').style.opacity = "1";
                }

                function DVP_stop() {
                  document.getElementById('DVP_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.11838">
                <papertitle>Blind Video Temporal Consistency via Deep Video Prior</papertitle>
              </a>
              <br>
              <strong>Chenyang Lei*</strong>,
              Yazhou Xing*,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>NeurIPS </em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2010.11838">arxiv</a> / 
              <a href="https://github.com/ChenyangLEI/deep-video-prior">code</a> / 
              <a href="DVP/index.html">project website</a>
              <p></p>
              <p>Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulted video. To address this issue,
                we present a novel and general approach for blind temporal video consistency. </p>
            </td>
          </tr>  

          <tr onmouseout="videodepth_stop()" onmouseover="videodepth_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='videodepth_image'>
                  <img src='images/depth_after.JPG' width="160"></div>
                <img src='images/depth_before.JPG' width="160">
              </div>
              <script type="text/javascript">
                function videodepth_start() {
                  document.getElementById('videodepth_image').style.opacity = "1";
                }

                function videodepth_stop() {
                  document.getElementById('videodepth_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.12874.pdf">
                <papertitle>Video Depth Estimation by Fusing Flow-to-Depth Proposals</papertitle>
              </a>
              <br>
              <a href="https://jiaxinxie97.github.io/">Jiaxin Xie</a>,
              <strong>Chenyang Lei</strong>,
              <a href="https://scholar.google.com/citations?user=gIBLutQAAAAJ&hl=en">Zhuwen Li</a>,
              <a href="http://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>IROS </em>, 2020  
              <br>
              <a href="https://arxiv.org/pdf/1912.12874.pdf">arXiv</a> / 
              <a href="https://github.com/jiaxinxie97/Video-depth-estimation">code</a> / 
              <a href="https://jiaxinxie97.github.io/Jiaxin-Xie/FDNet/FDNet">project website</a>
              <p></p>
              <p>We present an approach with a differentiable flowto-depth layer for video depth estimation.</p>
            </td>
          </tr>  

          <tr onmouseout="polar_rr_stop()" onmouseover="polar_rr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='polar_rr_image'>
                  <img src='images/polarRR_after.jpg' width="160"></div>
                <img src='images/PolarRR_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function polar_rr_start() {
                  document.getElementById('polar_rr_image').style.opacity = "1";
                }

                function polar_rr_stop() {
                  document.getElementById('polar_rr_image').style.opacity = "0";
                }
                polar_rr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cqf.io/papers/Polarized_Reflection_Removal_CVPR2020.pdf">
                <papertitle>Polarized Reflection Removal with Perfect Alignment in the Wild</papertitle>
              </a>
              <br>
              <strong>Chenyang Lei</strong>,
              Xuhua Huang,
              Mengdi Zhang,
              Qiong Yan,
              Wenxiu Sun,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>CVPR</em>, 2020  
              <br>
              <a href="https://cqf.io/papers/Polarized_Reflection_Removal_CVPR2020.pdf">arXiv</a> / 
              <a href="https://github.com/ChenyangLEI/CVPR2020-Polarized-Reflection-Removal-with-Perfect-Alignment">code</a> / 
              <a href="polar_rr/index.html">project website</a>

              <p></p>
              <p>Polarization information and perfect alignment are utilized to remove reflection accurately.</p>
            </td>
          </tr>  







          <tr onmouseout="porshadmanip_stop()" onmouseover="porshadmanip_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='porshadmanip_image'>
                  <img src='images/Color_before.jpg' width="160"></div>
                <img src='images/Color_after.jpg' width="160">
              </div>
              <script type="text/javascript">
                function porshadmanip_start() {
                  document.getElementById('porshadmanip_image').style.opacity = "1";
                }

                function porshadmanip_stop() {
                  document.getElementById('porshadmanip_image').style.opacity = "0";
                }
                porshadmanip_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cqf.io/papers/Fully_Automatic_Video_Colorization_CVPR2019.pdf">
                <papertitle>Fully Automatic Video Colorization with Self Regularization and Diversity</papertitle>
              </a>
              <br>
              <strong>Chenyang Lei</strong>,
              <a href="http://cqf.io">Qifeng Chen</a>
              <br>
              <em>CVPR</em>, 2019  
              <br>
              <a href="https://leichenyang.weebly.com/project-color.html">project page</a> / 
              <a href="https://www.youtube.com/watch?v=Y15uv2jnK-4&feature=youtu.be">video</a> /
              <a href="https://github.com/ChenyangLEI/Fully-Automatic-Video-Colorization-with-Self-Regularization-and-Diversity">code</a>
              <p></p>
              <p>The first dedicated video colorization method without any user input.</p>
            </td>
          </tr>  


        </tbody></table>



         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
                <ul>
                  <li>Program Committee/Reviewers: CVPR, ICCV, TPAMI, IJCV, AAAI, TIP, IJCAI, IROS, TVCG
                </ul>
            </td>
          </tr>
        </tbody></table>
        
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
                <ul>
                  <li>RedBird PhD Scholarship, HKUST, 2021</li>
                  <li>SENG Academic Award for Continuing PhD students, HKUST, 2020</li>
                  <li>National Scholarship, 2017</li>
                  <li>Outstanding Graduate (Zhejiang University), 2018</li>            
                  <li>Texas Instruments Scholarship, 2017</li>            
                  <li>First-Class Scholarship for Outstanding Merits, 2017</li>            
                  <li>Excellent Student Award, 2016, 2017</li>              
                </ul>              
            </td>
          </tr>
        </tbody></table>

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
                <ul>
                  <li>COMP 4901J: Deep Learning in Computer Vision (Spring 2019)</li>
                  <li>COMP 3031: Principle of Programming Languages (Fall 2019)</li>          
                  <li>COMP2011: Programming with C++ (Spring 2021)</li>          
                  
                </ul>
           </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">Thank <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing the source code of his personal page.</p>
                <!-- <br> -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?chenyang"
border="0" alt="Web Counters"></a>
<br><a href="https://www.easycounter.com/">Web Counters</a>

</html>
